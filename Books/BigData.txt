MapReduce provides a programming model that abstracts the problem from disk
reads and writes, transforming it into a computation over sets 
of keys and values. 
Hadoop provides a reliable, scalable, platform for storage analysis 
HBase: a key-value store that uses HDFS for its underlying storage.
HBase provides both online read/write access of individual rows and batch 
operations for reading and writing data in bulk. 

YARN : Yet another resource negotiator. YARN is a cluster resource management system
which allows any distributed program (not just Hadoop) to run on a Hadoop cluster.

Relational database management system vs MapReduce 
Hadoop works well on unstructured or semi-structured data because it
is designed to interpret the data at processing time (schema on read) 
A mapreduce job: input data, mapreduce program, configruation information 
Hadoop does its best to run the map task on a node where the input data 
resides in HDFS, because it does not use valuable cluster bandwidth (data locality optimization)

Filesystems that manage the storage across a network of machines are distributed filesystems
Node failure without suffering data loss. 
HDFS: write once, read many times. 
HDFS cluster has two types of nodes 
Namenode: master (manages the filesystem namespace). Maintains the filesystem tree and metadata for all the files and directories in the tree. 
DataNodes: workers. They store and retriee blocks when they are told to by 
the name node, and they report back to the namenode periodically
with lists of blocks that they are storing. 

If the nameode is obliterated, the filesystem cannot be used. 
All files on the filesystem would be lost since there would 
be no way of knowing how to reconstruct the files from the blocks
on the datanodes. 
Hadoop has two mechanisms to prevent namenode failure
1.) backup files that make up the persistent state of the filesystem metadata
Keep a secondary name node. It keeps a copy of the merged 
namespace image, and is run on a separate physical machine. 

Job scheudulers ( MapReduce, Spark, and other frameworks) 

How do we get the data into the system for processing? 
Scoop : tool to get data from relational databases into Hadoop (push in, push out).

Extract, transform, load (ETL)
FLUME (massively distribute framework, streaming data for HDFS)
:wq

