
HDFS : Hadoop’s data storage is different from relational databases in that it does not impose any data format or schema
Schemas change frequently. Someone refactors the code generating the JSON and moves fields around, perhaps renaming a few fields. The DBA adds new columns to a MySQL table and this reflects in the CSVs dumped from the table.

Apache AVRO is a data serialization project that provides schemas with rich data structures, compressible file formats, and simple integration with many programming languages.

How do the most common Hadoop File Formats stack up?
-Formats to avoid: any format that is not splittable should generally be avoided. The use of XML and JSON file formats is a common mistake.
-Text / CSV files. Csv files are common and often used for exchanging data between Hadoop and external systems. They are readable and parsable. However they do not support block compression, thus compressing a CSV file in Hadoop often comes at a significant read performance cost. No metadata should be stored with in the CSV. CSV files have limited support for schema evolution

AVRO files are the best multi-purpose storage format within Hadoop. Avro files can store metadata with the data but also allow independent schema for reading the file Avro files are splittable, support block compression, and enjoy broad, relatively mature, tool support within the Hadoop ecosystem.

Parquet Files are yet another columnar file format that originated from Hadoop creator Doug Cutting’s Trevni project. Like RC and ORC, Parquet enjoys compression and query performance benefits, and is generally slower to write than non-columnar file formats. However, unlike RC and ORC files Parquet series support limited schema evolution. In Parquet, new columns can be added at the end of the structure. At present, Hive and Impala are able to query newly added columns, but other tools in the ecosystem such as Hadoop Pig may face challenges. Parquet is supported by Cloudera and optimized for Cloudera Impala. Native Parquet support is rapidly being added for the rest of the Hadoop ecosystem.
 
One note on Parquet file support with Hive. It is very important that Parquet column names are lowercase. If your Parquet file contains mixed case column names, Hive will not be able to read the column and will return queries on the column with null values and not log any errors. Unlike Hive, Impala handles mixed case column names. A truly perplexing problem when you encounter it!

A data warehouse is a separate database that analysts can query to their hearts content without OLTP operation. Data warehouse contains a read-only copy of the data in all the various OLTP systems. Data is extracted from OLTP databases , transformed into an analysis friendly schema, cleansed up, and then loaded into a data warehouse. The process of getting loaded into the warehouse is ETL (extract, transform, load)

A big advantage of using a separate data warehouse rather than querying OLTP systems directly for analytics, is that the data warehouse can be optimized for analytic access patterns.

In most OLTP databases, storage is laid out in a row-oriented fashion : all the values from one row of a table are stored next to each other. Document databases are similar; an entire document is typically stored as contiguous sequence of bytes.

The idea behind column-oriented storage is simple: don’t store all the value from one row together, but store all the values from each column together instead. If each column is stored in a separate file. A query only needs to read and parse those columns that are used in that query, which can save a lot of work.

Column storage is easiest to understand in a relational data model, but it applies equally to non relational data

For data warehouse queries that need to scan over millions of rows, a big bottleneck is the bandwidth for getting data from disk into memory. Developers of analytical databases also worry about efficiently using the bandwidth from main memory into the CPU cache, avoiding branch mis-predictions and bubbles in the CPU instruction processing pipeline, and making use of single-instruction multi data instructions in modern CPUS

The compression effect is strongest on the first sort key. The second and third sort keys will be more jumbled up, and thus not have such long runs off repeated values. Columns further down the sorting priority appear in essentially random order, so they probably won’t compress as well.

Column oriented storage. Compression, and sorting all helps to make those read queries faster. Column oriented storage is commonly used in data warehousing. Data warehouse queries often involves aggregate functions (count, sum, avg, min, or max in SQL). If the same aggregates are used by many different queries, it can be wasteful to crunch through the raw data every time. Why not cache some of the counts or sums that queries use most often?

Standard (virtual view): a table like object whose contents are the results of some query.
The materialized view is an actual copy of the query results, written to disk, whereas a virtual view is just a shortcut for writing queries. When you read from a virtual view, the SQL engine expands it into the view’s underlying query on the fly and then processes the expanded query.

When the underlying data changes, a materialized view needs to be updated. This can be expensive, which is why materialized views are not often used in OLTP databases. In a read-heavy data warehouse they can make more sense.

A common special case of a materialized view is known as a data cube or OLAP cube. It is a grid of aggregates grouped together by different dimensions.

Materialized data cube (major advantage) is that certain queries can become very fast because why have effectively been precomputed. Disadvantage is the data cube does not have the same flexibility as querying the raw data. Most data warehouses therefore try to keep as much raw data as possible and use aggregates such as data cubes only as a performance boost for certain queries.

On a high level, we saw that storage engines fall into two broad categories: those optimized for transaction processing (OLTP), and those optimized for analytics (OLAP). There are big differences between the access patterns in those use cases:
* OLTP systems are typically user-facing, which means that they may see a huge volume of requests. In order to handle the load, applications usually only touch a small number of records in each query. The application requests records using some kind of key, and the storage engine uses an index to find the data for the requested key. Disk seek time is often the bottleneck here. 
* Data warehouses and similar analytic systems are less well known, because they are primarily used by business analysts, not by end users. They handle a much lower volume of queries than OLTP systems, but each query is typically very demanding, requiring many millions of records to be scanned in a short time. Disk bandwidth (not seek time) is often the bottleneck here, and column- oriented storage is an increasingly popular solution for this kind of workload. 
As an application developer, if you’re armed with this knowledge about the internals of storage engines, you are in a much better position to know which tool is best suited for your particular application. If you need to adjust a database’s tuning parameters, this understanding allows you to imagine what effect a higher or a lower value may have.

A change to an application’s feature also requires a change to data that it stores.
Relational databases generally assume that all data in the database conforms to one schema; although that schema can be changed

When a data format or schema changes, a corresponding change to application code often needs to happen
With server side applications you may want to perform a rolling upgrade (staged rollout), deploying the new version to a few nodes at a time, checking whether the new version is running smoothly, and gradually working your way through all the nodes. This allows all new versions to be deployed without service downtown, and thus encourages more frequent releases and better evolvability.

Backward compatibility: newer code can read data that was written by older code.
Forward compatibility: older code can read data that was written by newer code.

Formats for encoding data: In memory: data is kept In objects, structs, lists, arrays hash tables, trees and so on. These data structures are optimized for efficient access and manipulation by the CPU.
When you want to write data to a file or send it over the network, you have to encode it as some kind of self-contained sequence of bytes (ex: JSON document).

The translation from the in-memory representation to a byte sequence is  called encoding (serialization or marshaling) and the reverse is called decoding (parsing, deserialization).

XML is often criticized for being to verbose and unnecessarily complicated. JSON’s popularity is mainly due to its built-in support in web browsers.

JSON and XML have good support for Unicode character strings (human readable text) but they do not support binary strings (sequence of bytes without a character encoding).
CSV does not have any schema so it is up to the application to define the meaning of each row and column. IF an application change adds a new row or column you have to handle that change manually.


With avrò, when an application wants to encode some data (to write to a file or database, it send it over the network, it encodes the data using whatever version of the schema it knows about (WRITERS SCHEMA). When an application wants to decode some data (read it from a file or database, receive it from the network), it is expecting the data to be in some schema (READERS schema).

Reading schema and writing schema do not have to be the same - they just have to be compatible. When data is decoded (read) the Avro library resolves the differences by looking at the writer’s schema and the reader’s schema side by side and translating the data from the writer’s schema into the reader’s schema.

It's no problem if the writer’s schema and the reader’s schema have their fields in different order, because the schema resolution matches up the fields by field name. If the coding reading the data encounters a field that appears in the writer’s schema but not in the readers schema, it is ignored. If the code reading the data expects some field, bu the writer’s schema does not contain a field of that name, it is filled in with a default value declared in the reader’s schema.

To maintain compatibility, you may add or remove field that has a default value. For example, say you add a field with a default value, so this new field exists in the new schema but not the old one. When a reader using the new schema reads a record written with the old schema, the default value is filled in father missing value.

A database of schema versions is useful thing to have in any case, since it acts as documentation and gives you a chance to check schema compatibility.

Schemas describe a binary encoding format.
Keeping a database of schemas allows you to check forward and backward compatibility of schema changes before anything is deployed.

A key design goal of a service-oriented / micro services architecture is to make the application easier to change and maintain by making services independently deployable and evolvable. Each service should be owned by one team, and that team should be able to release new versions of the service frequently without having to coordinate with other teams.

A data-intensive application is built from standard building blocks that provide:
Store data so that they, or another application can find it again later (databases)
Remember the result of an expensive operation, to speed up reads (caches)
Allow users to search data by keyword or filter it in various ways
(Search indexes)
Send a message to another process, to be handled asynchronously (stream processing)
Periodically crunch a large amount of accumulated data (batch processing)

A fault is usually defined as one component of the system deviating from its spec, whereas a failure is when the system as a whole stops providing the required service to the user.
It is best to design fault tolerance mechanisms that prevent faults from causing failures.

Shared-nothing architectures ( horizontal scaling or scaling out) have gained a lot of popularity. Each machine or virtual machine running the database software is called a node. Each node uses its CPUs, RAM, and disks independently. Any coordination between nodes is don’t at the software level.

DISTRIBUTED DATA SYSTEMS

Replications, partitioning ( splitting a big database into smaller subsets so that different partitions can be assigned to different nodes).

There are three different types fo replication : single-leader, multi-leader, and leaderless replication

Each node that stores a copy of the database is a replica. Every write to the database needs to be processed by ever replica. (Leader-based replication or active / passive or master - slave replication

One of the replicas is master. Master pings all followers that there was a change and that  it needs to apply that change. When a client wants to read from the database it can query either the leader any of the followers. However writes can only be accepted by the leader.

Asynchronous vs synchronous replication
Synchronous replication: major benefit is that the followers are guaranteed to have an up to date copy of the data that is consistent with the leader. The synchronous checks and waits for a response before proceeding forward. If the synchronous follower does not respond, the write cannot be processed. The leader must block all writes and wait until the synchronous replica is available again.

This is impractical because any one node outage would cause the whole system to be halted. In practice:
In practice, if you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. This guarantees that you have an up-to-date copy of the data on at least two nodes: the leader and one synchronous follower. This configuration is sometimes also called semi-synchronous

Failover can happen. An automatic failover process usually looks like this:
Determining that the leader has failed. Most systems simply use timeout: nodes frequently bounce messages back and forth between each other, and if a node does not respond for some period of time (30 seconds) it assumed dead.
Choosing a new leader. (A new leader can be appointed by an election process. The best candidate for leadership is usually the replica with the most up to date data changes from the old leader (to minimize any data loss).
Reconfiguring the system to use the new leader.

If an asynchronous replication is used, the new leader may not have received all the writes from the old leader before it failed. Discarding writes is dangerous if other storage systems outside of the database need to be coordinated with the database contents.

Split brain. This is when two nodes become leaders. If both leaders accept writes, there is no process for resolving conflicts, data is likely to be lost or corrupted. Some systems have a mechanism to shut down one node if two leaders detected.


-------------------------------------------------------------------------------

MapReduce provides a programming model that abstracts the problem from disk
reads and writes, transforming it into a computation over sets
of keys and values.
Hadoop provides a reliable, scalable, platform for storage analysis
HBase: a key-value store that uses HDFS for its underlying storage.
HBase provides both online read/write access of individual rows and batch
operations for reading and writing data in bulk.

YARN : Yet another resource negotiator. YARN is a cluster resource management system
which allows any distributed program (not just Hadoop) to run on a Hadoop cluster.

Relational database management system vs MapReduce
Hadoop works well on unstructured or semi-structured data because it
is designed to interpret the data at processing time (schema on read)
A mapreduce job: input data, mapreduce program, configruation information
Hadoop does its best to run the map task on a node where the input data
resides in HDFS, because it does not use valuable cluster bandwidth (data locality optimization)

Filesystems that manage the storage across a network of machines are distributed filesystems
Node failure without suffering data loss.
HDFS: write once, read many times.
HDFS cluster has two types of nodes
Namenode: master (manages the filesystem namespace). Maintains the filesystem tree and metadata for all the files and directories in the tree.
DataNodes: workers. They store and retrieve blocks when they are told to by
the name node, and they report back to the namenode periodically
with lists of blocks that they are storing.

If the nameode is obliterated, the filesystem cannot be used.
All files on the filesystem would be lost since there would
be no way of knowing how to reconstruct the files from the blocks
on the data nodes.
Hadoop has two mechanisms to prevent namenode failure
1.) backup files that make up the persistent state of the filesystem metadata
Keep a secondary name node. It keeps a copy of the merged
namespace image, and is run on a separate physical machine.

Job scheudulers ( MapReduce, Spark, and other frameworks)

How do we get the data into the system for processing?
Scoop : tool to get data from relational databases into Hadoop (push in, push out).

Extract, transform, load (ETL)
FLUME (massively distribute framework, streaming data for HDFS)

-----------------------------------------------------------------------
What are the top priorities for this company and what are we going after? What technology or product trends are opening up new opportunities? Who are our customers? What is the market, and how could I do this differently with data?
-----------------------------------------------------------------------

--- Spark --- 
Resilient Distributed Dataset: Spark automatically distributes the data
contained in RDDs across the cluster and parallelizes the opeartions you 
perform on time. An RDD is an immutable disrributed collection of objects.
Each RDD is split into multiple partitions, which may be computed 
on different nodes of the cluster. RDDs can contain any type of Python, Java, 
Scala objects including user defined classes. 

Every Spark program and shell program will work as follows:
Create some input RDDs from external data, 
Transform them to define new RDDs using transformations like filter
Add spark to persist() any immediate RDDs that will need to be reused 
Launch applications such as count() and first() to kick off a parallel
computation which is often optimized and executed by spark. 
Meta data: Data that describes other data. It is the underlying definition or
description of data. 

Spark Structured data (schema): consistent set of fields across data nodes.
Spark SQL supports multiple structured data sources as input and because 
it understand their schema, it can effectively read only the fields you require from these data sources. 

RDDs are fault tolerant, parallel data structures that let users explicitly 
persist immediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators. 

---- HIVE ----
Apache HIVE: Hive can store tables in a variety of formats (plain-text to column
oriented formats, inside HDFS or other storage systems). Spark SQL can load any table any table supported by HIVE. 

In HIVE there are two ways to create a table (managed and external tables) 
When we create a table in HIVE, it manages and saves it in its own warehouse. When we load 
data into a managed table, it is moved into HIVE warehouse directory 
create table managed_table(ds_string); 
load data inpath "pathName"
Now this load statement will move the file into the HIVE warehouse drectory for the manager_table
If the table is later dropped: The table including its metadata and its data is deleted, which means the data no longer exists. 
External tables: in case of external tables, tge location of the external data is specified at table creation time and also uses the key word external in create statement. 

If you are doing all your processing in HIVE then it is a good idea to use a managed table,
where as incase we use HIVE as well as another tool on the same dataset then it is godo to use
an external table or incase of we use multiple schemas for the same data set. 
Use external tables when the data is used outside of HIVE.
External tables store files on the HDFS server but tables are not linked to the source file 
completely. If you delete an external table the file still remains on the HDFS server. 
External table files are accessible to anyone who has access to HDFS file structure and therefore 
security needs to be maaged at the HDFS file / folder level. 
Meta data is maintained on master node, and deleting an external table from HIVE only delets the metadata
not the data / file. 

Metastore is the central repostiroy of Apache Hive metadata
It stores metadata for tables like (schema and location) / partitions 
for relational databases. It provides client access to this information by using a meta store API 


--- Impala -- 
Massive parallel SQL engine that runs on a Hadoop cluster. Can query date stored in HDFS or Hbase tables.
High performance. Faster than HIVE. 
Hive and Impala work with the same data (tables in HDFS, metadata in the metastore)
HIve and Imapala are tools for performing SQL queries on data in Hadoop 
HIVE generates jobs that run on the Hadoop cluster data processing pipeline 
Impala executes queries directly on the Hadoop cluster

Data partitioning: By default all data files for a table are stored in a single directory
Partitioning subdivides the data. Data is physically divided during loading, based on values from
one value or more columns. Speeds up queries that filter on partition columns. Loading data into a 
partitioned table. Use partitioning for tables when:
reading the entire data sets takes to long; queries almost always filter on the partition columns;
There are a reasonable number of different values for partition columns; Data generator or ETL
process splits data by file or directory names; partition column values are not in the data itself.

Tokenization: process of replacing sensitive data with unique identification symbols that retain
essiential information about the data without compromising security. The token is an identifer that 
maps back to the sensitive data through a tokenization system.
Encyption: security information where information is encoded and can only be accessed or decrypted 
by a user with the correct encryption key. Used to deter malicious or negligent parties from 
accessing sensitive data. (asymmetric vs symmetric encrption): Encrpytion is the conversion of 
data from readable format into an encoded format that can only be read or processed fter it's been decrypted. 
Summarization: process of creating concise, informative version of the original data.
Data Enrichment: managing third-party data from an external authoritative source with an existing 
database of first party customer data. General term that refers to processes used to enhance, refine, or otherwise improve raw data. 

HashMaps will do resizing after there is a lot of key-value pairs 
Hadoop: 
Volume: size of how much data you have
Velocity: How fast data is getting to you 
Variety: How different your data is 
Veracity: Credability, how reliable your data is 

Hadoop is a platform for distributed storing and analyzing of very large data sets 
Four main modules: Hadoop Common, HDFS, MapReduce, Yarn, Ecosystem For Storing / Processing data 
Hive creates temporary folders when executing a job. After the job is completed the temporary folder is deleted

https://github.com/andkret/Cookbook/blob/master/AdvancedSkills.md  <-- breaks down MapReduce / Spark 
https://github.com/andkret/Cookbook/blob/master/InterviewQuestions.md 

Sqoop is a tool used to transfer data between RDBMS and HDFS 
If a data node fails the job tracker and the name node will detect the failure then deallocate the node 
Truncating a table: delete a table from the database and memory while keeping the structure
of the table intact 

